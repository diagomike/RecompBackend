I want to build a highly versatile task runner in python that manages a queue of tasks and the tasks themselves are independent functional implementations of python (mostly machine learning model running heavy programs like transcription, face tracking detection and so on) we can maybe implement an abstract class or interface and or adapter design pattern to help standardize communication between the task runner and the independent modules

Since I want the task runner to be used be web applications the services should be available through an API and web interfaces (applications) can run multiple of these massive backend processed that the task runner queues and manages their execution (the execution of tasks is obviously on a separate thread in the backend then the main thread that the task runner and api hosts are running on - just to not make the backend freeze as the processes are heavy duty)

The Inputs to the functions and the results from their execution are to be saved in a database (I want to use mongoDB) and in a separate file management structure - because I will mostly work with Images, Videos and their transcripts - the Heavy duty functions take inputs and generate results which and these are also communicable through API (i.e. people can access files (video, image, audio, transcript text) that they uploaded and the results of the processing done on them by the heavy functions - (reprocessed video, generated audio, generated image or transcript text - text can be directly saved in database) - every data we are working with should be labeled in the database and if it is a file instance then the API would know to forward the file data from the data storage - not just forward the data available in the database

I think The processing procedure should be like - upload the data that you want to be used as inputs to the heavy duty functions and then once data are available in the database you can create a task and assign the labeled/uploaded input data to the task - and this task creation will generate and label the output data (result of the processing) with a pending status - the result can be used as input to other or new tasks - but those tasks will be blocked/moved back to queue - until the predecessor task is finished with a success status and it’s outputs data status moves from pending to available.

The API should also be able to provide information on the tasks (active, completed and planned tasks) for dashboard information purposes - and also enable access to available files whether they are uploaded or generated by heavy processing - these could also be data directly from database - like subtitle text  - (the data to be stored should maybe have a type of isFile - which indicates that the content is either directly in database or the content key points to a location in the local file management folder structure)

The task runner should be built independently from the modules it runs - and it should a way to incorporate any new module that is added (maybe I have to move the modules files into the modules folder with a main.py file in it - and then I call a detect-modules.py that auto detects and incorporates the modules into the available functions list) - and obviously every module on insertion should have a test-case and setup runner - to maybe install the requirements on the current machine and a dummy test case to run with fed inputs and see that it returns the desired kind of output and only when it passes will it be integrated into the task-runners library of available functions - this will register it on the database for availability - the test to run and requirements to install should be required in the adapter or interface that the modules need to adhere to to be eligible for integration into the task runner

This means the frontend that is going to access the task runner through an API should have detect-module and see available modules management ability provided to its interface

– 
We will later add basic modules to test the working of the queue, the interface and status and data management - but first we want to build the highly versatile task runner which should work in the above provided constraints

